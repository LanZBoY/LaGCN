{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MuliLayerTextGCN\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from model import Vocaburary\n",
    "# from time import time\n",
    "# from datetime import timedelta\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_layers = [i for i in range(2, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 300\n",
    "HIDDEN_DIM = 200\n",
    "\n",
    "dataset_names = {\n",
    "    \"20NewsGroup\": \"20NG\",\n",
    "    \"MR\":\"mr\",\n",
    "    \"Ohsumed\":\"ohsumed_single_23\",\n",
    "    \"R52\":\"R52\",\n",
    "    \"R8\":\"R8\"\n",
    "}\n",
    "\n",
    "result = {k : {} for k in dataset_names.keys()}\n",
    "\n",
    "SAVE_PATH = './result/multi_layer.result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(SAVE_PATH):\n",
    "    for key in dataset_names.keys():\n",
    "        print(\"===================================\")\n",
    "        print(key)\n",
    "        dir_name = dataset_names[key]\n",
    "\n",
    "        dict_data = torch.load(f'./ProcessedData/{dir_name}/WholeGraphDict.gh')\n",
    "        voc : Vocaburary = dict_data['voc']\n",
    "        whole_graph = dict_data['whole_graph'].cuda()\n",
    "        word_num = dict_data['W']\n",
    "        label_num = dict_data['L']\n",
    "        doc_num = dict_data['D']\n",
    "        train_mask = dict_data['train_mask'].cuda()\n",
    "        doc_Y : torch.Tensor = dict_data['doc_Y'].cuda()\n",
    "        word_Y : torch.Tensor = dict_data['word_Y'].T.cuda()\n",
    "        label_Y : torch.Tensor = dict_data['label_Y'].cuda()\n",
    "        train_words = list(dict_data['train_word'])\n",
    "        test_words = list(dict_data['test_word'])\n",
    "        train_words.sort()\n",
    "        test_words.sort()\n",
    "        train_num = train_mask.count_nonzero().cpu().item()\n",
    "        test_num = doc_num - train_num\n",
    "\n",
    "        result[key]['statistic'] = {\n",
    "        \"#DOC\":doc_num,\n",
    "        \"#Word\":word_num,\n",
    "        \"#Class\":label_num,\n",
    "        \"#Train\" : train_num,\n",
    "        \"#Test\" : test_num,\n",
    "        \"#NODE\" : word_num + doc_num + label_num\n",
    "        }\n",
    "        result[key][\"N_ACC\"] = []\n",
    "        result[key][\"N_layers\"] = N_layers\n",
    "        for N_layer in result[key][\"N_layers\"]:\n",
    "            model = MuliLayerTextGCN(whole_graph.shape[0], HIDDEN_DIM, label_num, N_layer=N_layer).cuda()\n",
    "            optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            trainingProcess = tqdm(range(EPOCH))\n",
    "            test_accs = []\n",
    "            for epoch in trainingProcess:\n",
    "                total_loss = 0.\n",
    "                optim.zero_grad()\n",
    "                y_hat = model(whole_graph)\n",
    "                doc_Y_hat = y_hat[:doc_num]\n",
    "                word_Y_hat = y_hat[doc_num:-label_num]\n",
    "                label_Y_hat = y_hat[doc_num+word_num :]\n",
    "                doc_loss = loss_fn(doc_Y_hat[train_mask], doc_Y[train_mask])\n",
    "                word_loss = loss_fn(word_Y_hat[train_words], word_Y[train_words])\n",
    "                label_loss = loss_fn(label_Y_hat, label_Y)\n",
    "                loss = 1.0 * doc_loss + 1.0 * word_loss  + 1.0 * label_loss\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                loss_val = loss.item()\n",
    "                with torch.no_grad():\n",
    "                    acc_val = ((doc_Y_hat.argmax(1)[~train_mask] == doc_Y.cuda()[~train_mask]).sum() / (~train_mask).sum()).item()\n",
    "                trainingProcess.set_postfix({\"LOSS\": loss_val, \"ACC\" : acc_val})\n",
    "                test_accs.append(acc_val)\n",
    "            result[key]['N_ACC'].append(max(test_accs))\n",
    "    with open(SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "else:\n",
    "    with open(SAVE_PATH, 'rb') as f:\n",
    "        result = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = {}\n",
    "processed_data['N_layers'] = N_layers\n",
    "for dataset, data in result.items():\n",
    "    processed_data[dataset] = list(map(lambda x : x * 100.,data['N_ACC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(processed_data)\n",
    "df = df.reindex(columns=['N_layers','MR', 'R8', 'R52', 'Ohsumed', '20NewsGroup'])\n",
    "df = df.rename(columns={\"20NewsGroup\" : \"20NG\", \"N_layers\" : \"Layer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./result/multi_layer.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
