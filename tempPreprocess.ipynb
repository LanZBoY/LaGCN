{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wen2Tee5\\Desktop\\final\\finalEnv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import dgl.sparse as dglsp\n",
    "from nltk.corpus import stopwords\n",
    "from utils import clean_str, remove_stopwords, nomalize_Adj\n",
    "from model import Vocaburary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = './ProcessedData'\n",
    "TRAIN_DATA = 'train_data.csv'\n",
    "TEST_DATA = 'test_data.csv'\n",
    "TARGET_DATA = 'WholeGraphDict.gh'\n",
    "WINDOW_SIZE = 20\n",
    "dataset = 'mr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset = mr\n"
     ]
    }
   ],
   "source": [
    "print(f'Current Dataset = {dataset}')\n",
    "file_path = os.path.join(ROOT, dataset)\n",
    "train_df = pd.read_csv(os.path.join(file_path, TRAIN_DATA), index_col=False)\n",
    "test_df = pd.read_csv(os.path.join(file_path, TEST_DATA), index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting Word...: 100%|██████████| 7108/7108 [00:00<00:00, 253441.75it/s]\n",
      "Counting Word...: 100%|██████████| 3554/3554 [00:00<00:00, 273309.19it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['text'] = train_df['text'].map(clean_str)\n",
    "test_df['text'] = test_df['text'].map(clean_str)\n",
    "if dataset != 'mr':\n",
    "    train_df['text'] = train_df['text'].map(remove_stopwords)\n",
    "    test_df['text'] = test_df['text'].map(remove_stopwords)\n",
    "\n",
    "word_count = {}\n",
    "for text in tqdm(train_df['text'], desc='Counting Word...'):\n",
    "    for word in text.split():\n",
    "        if word not in word_count:\n",
    "            word_count.update({word: 0})\n",
    "        word_count[word] +=1\n",
    "for text in tqdm(test_df['text'], desc='Counting Word...'):\n",
    "    for word in text.split():\n",
    "        if word not in word_count:\n",
    "            word_count.update({word: 0})\n",
    "        word_count[word] +=1\n",
    "if dataset == 'mr':\n",
    "    voc = Vocaburary(word_count=word_count)\n",
    "else:\n",
    "    voc = Vocaburary(word_count=word_count, min_time = 5)\n",
    "\n",
    "train_df['ids'] = train_df['text'].map(lambda x : voc.encode(x.split()))\n",
    "test_df['ids'] = test_df['text'].map(lambda x : voc.encode(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "constrcuting train_word_set...: 100%|██████████| 7108/7108 [00:00<00:00, 789688.58it/s]\n",
      "constructing test_word_set...: 100%|██████████| 3554/3554 [00:00<00:00, 645221.68it/s]\n"
     ]
    }
   ],
   "source": [
    "train_word_set = set()\n",
    "test_word_set = set()\n",
    "for ids in tqdm(train_df['ids'], desc='constrcuting train_word_set...'):\n",
    "    id_set = set(ids)\n",
    "    train_word_set.update(ids)\n",
    "for ids in tqdm(test_df['ids'], desc='constructing test_word_set...'):\n",
    "    id_set = set(ids)\n",
    "    test_word_set.update(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_num = doc_id + 1\n",
    "label_num = train_df['target'].unique().max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7108it [00:00, 710699.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_word_label = {\n",
    "    'label_node' : [],\n",
    "    'word_node': []\n",
    "}\n",
    "for label_id, ids in tqdm(zip(train_df['target'], train_df['ids'])):\n",
    "    data = ids\n",
    "    train_word_label['label_node'] += [label_id for _ in data]\n",
    "    train_word_label['word_node'] += data\n",
    "label_word_mat = torch.sparse_coo_tensor(\n",
    "    indices=[train_word_label['label_node'], train_word_label['word_node']],\n",
    "    values=[1. for _ in range(len(train_word_label['word_node']))],\n",
    "    size=(label_num, len(voc))\n",
    ")\n",
    "countMat = label_word_mat.coalesce().to_dense()\n",
    "train_word = (countMat / countMat.T.sum(dim=1)).nan_to_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.2500, 1.0000,  ..., 0.4545, 1.0000, 0.4815],\n",
       "        [0.6667, 0.7500, 0.0000,  ..., 0.5455, 0.0000, 0.5185]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
